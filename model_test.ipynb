{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from sentence_transformers import util\n",
    "\n",
    "def change_cha(word:str)->str:\n",
    "    # 对于每个词，随机选择一个位置，将该位置的字母替换为另一个字母\n",
    "    word = list(word)\n",
    "    index = random.randint(0,len(word)-1)\n",
    "    word[index] = random.choice(string.ascii_lowercase)\n",
    "    return ''.join(word)\n",
    "\n",
    "def add_cha(word:str)->str:\n",
    "    # 对于每个词，随机选择一个位置，在该位置插入一个字母\n",
    "    word = list(word)\n",
    "    index = random.randint(0,len(word))\n",
    "    word.insert(index,random.choice(string.ascii_lowercase))\n",
    "    return ''.join(word)\n",
    "\n",
    "def delete_cha(word:str)->str:\n",
    "    # 对于每个词，随机选择一个位置，删除该位置的字母\n",
    "    word = list(word)\n",
    "    index = random.randint(0,len(word)-1)\n",
    "    word.pop(index)\n",
    "    return ''.join(word)\n",
    "\n",
    "def swap_cha(word:str)->str:\n",
    "    # 对于每个词，随机选择两个位置，交换这两个位置的字母\n",
    "    word = list(word)\n",
    "    index1 = random.randint(0,len(word)-1)\n",
    "    index2 = random.randint(0,len(word)-1)\n",
    "    word[index1], word[index2] = word[index2], word[index1]\n",
    "    return ''.join(word)\n",
    "\n",
    "def change_data(data:pd.DataFrame)->pd.DataFrame:\n",
    "    data_label = data['s2'].reset_index(drop=True)\n",
    "    \n",
    "    # 对于每一行，将文本根据空格进行分割，分割后随机选择1-2个词，将单词切分后随机在一个位置填入一个字母，再讲字母重新拼接成单词\n",
    "    data_text = data_label.apply(lambda x: x.split())\n",
    "    for text in data_text:\n",
    "        # change_or_not = random.random()\n",
    "        # if change_or_not <= 0.1:\n",
    "        for i in range(len(text)):\n",
    "            word_change_or_not = random.random()\n",
    "            if word_change_or_not <= 0.1 and len(text[i]) > 1:\n",
    "                change_method = random.random()\n",
    "                if change_method < 0.25:\n",
    "                    text[i] = change_cha(text[i])\n",
    "                elif change_method < 0.5:\n",
    "                    text[i] = add_cha(text[i])\n",
    "                elif change_method < 0.75:\n",
    "                    text[i] = delete_cha(text[i])\n",
    "                else:\n",
    "                    text[i] = swap_cha(text[i])\n",
    "    data_text = data_text.apply(lambda x: ' '.join(x))\n",
    "    data['s2'] = data_text.copy()\n",
    "\n",
    "    return data\n",
    "\n",
    "def mask_data(data:pd.DataFrame)->pd.DataFrame:\n",
    "    data_label = data['s2'].reset_index(drop=True)\n",
    "    \n",
    "    data_text = data_label.apply(lambda x: x.split())\n",
    "    for text in data_text:\n",
    "        # mask_or_not = random.random()\n",
    "        # if mask_or_not <= 0.1:\n",
    "        for i in range(len(text)):\n",
    "            word_mask_or_not = random.random()\n",
    "            if word_mask_or_not <= 0.1 and len(text[i]) > 1:\n",
    "                text[i] = ' '\n",
    "    data_text = data_text.apply(lambda x: ' '.join(x))\n",
    "\n",
    "    data['s2'] = data_text.copy()\n",
    "    \n",
    "    return data\n",
    "\n",
    "def expand_data(data_refer:pd.DataFrame, data:pd.DataFrame)->pd.DataFrame:\n",
    "    data = data.reset_index(drop=True)\n",
    "    text_code = data['code_sum']\n",
    "    data_text = data['s2'].copy()\n",
    "    for index_code, code in enumerate(text_code):\n",
    "        # expand_or_not = random.random()\n",
    "        # if expand_or_not <= 0.1:\n",
    "        # 从data_refer中找到code_sum列包含code的行\n",
    "        code = re.escape(code)\n",
    "        data_refer_text = data_refer[data_refer['code_sum'].str.contains(code)]['narr_accf']\n",
    "        if (data_refer_text.empty):\n",
    "            continue\n",
    "        # 随机选择一个行，将该行的narr_accf列的文本添加到原文本后面\n",
    "        refer_text = data_refer_text.sample(1).iloc[0]\n",
    "        orig_text = data_text.loc[index_code]\n",
    "        data_text.loc[index_code] = orig_text + '。' + refer_text\n",
    "    data['s2'] = data_text.copy()\n",
    "    return data\n",
    "\n",
    "def find_max_index(embeddings1, embeddings2):\n",
    "    # 计算两个tensor之间的余弦相似度\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "    # 寻找tensor最大值的行号与列号\n",
    "    values, indices = cosine_scores.max(dim=0)\n",
    "    max_index = indices[values.argmax()].tolist()\n",
    "    return max_index\n",
    "\n",
    "def fliter(embeddings0, embeddings2):\n",
    "    # 寻找最相关的事故发生阶段与事故发生原因\n",
    "    index0 = find_max_index(embeddings0, embeddings2)\n",
    "    embeddings = embeddings0[index0]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入并预处理语料库，创建语料库的句向量\n",
    "data = pd.read_csv('./data/describe_sum_carrier.csv', encoding = 'utf-8')\n",
    "data = pd.DataFrame(data)\n",
    "keyword = data['Occurrence_description_sum'].tolist()\n",
    "ntsb_no = data['ntsb_no'].tolist()\n",
    "refer = data['narr_accf'].tolist()\n",
    "if os.path.exists('./data/refer_embeddings.pkl'):\n",
    "    with open('./data/refer_embeddings.pkl', 'rb') as f:\n",
    "        refer_embeddings = pickle.load(f)\n",
    "else:\n",
    "    refer_new = [p(sentence) if len(sentence)<=512 else p(sentence[:128] + sentence[-384:]) \n",
    "                for sentence in refer]\n",
    "    refer_embeddings = model.encode(refer_new, convert_to_tensor = True)\n",
    "    pickle.dump(refer_embeddings, open('./data/refer_embeddings.pkl', 'wb'))\n",
    "# 事故发生阶段+原因\n",
    "if os.path.exists('./data/embeddings0.pkl'):\n",
    "    with open('./data/embeddings0.pkl', 'rb') as f:\n",
    "        embeddings0 = pickle.load(f)\n",
    "else:\n",
    "    dict = pd.read_csv('./data/describe_category_carrier.csv')\n",
    "    dict = [phase for phase in dict['Occurrence_description_sum']]\n",
    "    embeddings0 = model.encode(dict, convert_to_tensor = True)\n",
    "    #存为pkl文件\n",
    "    pickle.dump(embeddings0, open('./data/embeddings0.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "string_test = 'Cruise - normal In flight encounter with weather'\n",
    "data = pd.DataFrame({'Occurrence_description_sum':[string_test]})\n",
    "data['code_sum'] = '541+240'\n",
    "embeddings2 = model.encode(data['Occurrence_description_sum'].tolist(), convert_to_tensor = True)\n",
    "expand_st = fliter(embeddings0, embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 s2    label\n",
      "0  Cruism - normal In flight encounter with weather  541+240\n"
     ]
    }
   ],
   "source": [
    "# data_refer = pd.read_csv('./data/describe_sum_carrier.csv')\n",
    "string_test = 'Cruise - normal In flight encounter with weather'\n",
    "data = pd.DataFrame({'s2':[string_test]})\n",
    "data['label'] = '541+240'\n",
    "change_st = change_data(data)\n",
    "print(change_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "module"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "type(pickle._Unpickler)\n",
    "type(torch.nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0+cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-cased/resolve/main/vocab.txt (Caused by ProxyError('Cannot connect to proxy.', OSError(0, 'Error')))\"), '(Request ID: 173bc47a-4956-4bca-8713-fcd52189b3f2)')' thrown while requesting HEAD https://huggingface.co/bert-base-cased/resolve/main/vocab.txt\n"
     ]
    },
    {
     "ename": "ProxyError",
     "evalue": "(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-cased/resolve/main/vocab.txt (Caused by ProxyError('Cannot connect to proxy.', OSError(0, 'Error')))\"), '(Request ID: 173bc47a-4956-4bca-8713-fcd52189b3f2)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    711\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_new_proxy_conn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhttp_tunnel_required\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_proxy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_prepare_proxy\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m         \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    368\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtls_in_tls_required\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_connect_tls_proxy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhostname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m                 \u001b[0mtls_in_tls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_connect_tls_proxy\u001b[1;34m(self, hostname, conn)\u001b[0m\n\u001b[0;32m    509\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhostname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m             \u001b[0mssl_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mssl_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m         )\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\urllib3\\util\\ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    452\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m         \u001b[0mssl_sock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ssl_wrap_socket_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtls_in_tls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mssl_sock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\urllib3\\util\\ssl_.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 495\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m             \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m         )\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\ssl.py\u001b[0m in \u001b[0;36m_create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m    869\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    871\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1138\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 0] Error",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    496\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m                 \u001b[0mchunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    498\u001b[0m             )\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    801\u001b[0m             retries = retries.increment(\n\u001b[1;32m--> 802\u001b[1;33m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    803\u001b[0m             )\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-cased/resolve/main/vocab.txt (Caused by ProxyError('Cannot connect to proxy.', OSError(0, 'Error')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProxyError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29568\\452593953.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mneuspell\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertChecker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 初始化并加载预训练模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mchecker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertChecker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mchecker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"neuspell_bert\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\neuspell\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# from .corrector_aspell import CorrectorAspell as AspellChecker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# from .corrector_jamspell import CorrectorJamspell as JamspellChecker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcorrector_bertsclstm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCorrectorBertSCLstm\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mBertsclstmChecker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcorrector_cnnlstm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCorrectorCnnLstm\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mCnnlstmChecker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcorrector_lstmlstm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCorrectorLstmLstm\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mNestedlstmChecker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\neuspell\\corrector_bertsclstm.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcommons\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mARXIV_CHECKPOINTS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCorrector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mseq_modeling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbertsclstm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_pretrained\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_inference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mseq_modeling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloads\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdownload_pretrained_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mseq_modeling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhelpers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbert_tokenize_for_valid_examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\neuspell\\seq_modeling\\bertsclstm.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mevals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mhelpers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertSCLSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\neuspell\\seq_modeling\\helpers.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;31m# import torch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[1;31m# from torch.nn.utils.rnn import pad_sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 609\u001b[1;33m \u001b[0mBERT_TOKENIZER\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert-base-cased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    610\u001b[0m \u001b[0mBERT_TOKENIZER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[0mBERT_TOKENIZER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize_chinese_chars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1796\u001b[0m                     \u001b[0m_raise_exceptions_for_missing_entries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1797\u001b[0m                     \u001b[0m_raise_exceptions_for_connection_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1798\u001b[1;33m                     \u001b[0m_commit_hash\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcommit_hash\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1799\u001b[0m                 )\n\u001b[0;32m   1800\u001b[0m                 \u001b[0mcommit_hash\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_commit_hash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolved_vocab_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommit_hash\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m             \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m         )\n\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1197\u001b[0m                     \u001b[0mtoken\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m                     \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1199\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1200\u001b[0m                 )\n\u001b[0;32m   1201\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mEntryNotFoundError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhttp_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[0;32m   1537\u001b[0m         \u001b[0mfollow_relative_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1538\u001b[0m         \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1539\u001b[1;33m         \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1540\u001b[0m     )\n\u001b[0;32m   1541\u001b[0m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[0mfollow_relative_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m         )\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[0mretry_on_status_codes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m         \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m     )\n\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\huggingface_hub\\utils\\_http.py\u001b[0m in \u001b[0;36mhttp_backoff\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnb_tries\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmax_retries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;31m# Sleep for X seconds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\huggingface_hub\\utils\\_http.py\u001b[0m in \u001b[0;36mhttp_backoff\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m             \u001b[1;31m# Perform request and return if status_code is not in the retry list.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mretry_on_status_codes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    587\u001b[0m         }\n\u001b[0;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\huggingface_hub\\utils\\_http.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;34m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mrequest_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_AMZN_TRACE_ID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\ragfliter\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ProxyError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 513\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mProxyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_SSLError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProxyError\u001b[0m: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-cased/resolve/main/vocab.txt (Caused by ProxyError('Cannot connect to proxy.', OSError(0, 'Error')))\"), '(Request ID: 173bc47a-4956-4bca-8713-fcd52189b3f2)')"
     ]
    }
   ],
   "source": [
    "from neuspell import BertChecker\n",
    "\n",
    "# 初始化并加载预训练模型\n",
    "checker = BertChecker()\n",
    "checker.from_pretrained(\"./neuspell_bert/\")\n",
    "\n",
    "# 进行拼写检查\n",
    "text = \"This is an exmple of a text with speling erors.\"\n",
    "corrected_text = checker.correct(text)\n",
    "\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Corrected: {corrected_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocab from path:d:\\anaconda\\Lib\\site-packages\\neuspell\\../data/checkpoints/subwordbert-probwordnoise\\vocab.pkl\n",
      "initializing model\n",
      "SubwordBert(\n",
      "  (bert_dropout): Dropout(p=0.2, inplace=False)\n",
      "  (bert_model): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dense): Linear(in_features=768, out_features=100002, bias=True)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "185211810\n",
      "loading pretrained weights from path:d:\\anaconda\\Lib\\site-packages\\neuspell\\../data/checkpoints/subwordbert-probwordnoise\n",
      "Loading model params from checkpoint dir: d:\\anaconda\\Lib\\site-packages\\neuspell\\../data/checkpoints/subwordbert-probwordnoise\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'CustomUnpickler' has no attribute 'Unpickler'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Your existing code to initialize and use the checker\u001b[39;00m\n\u001b[0;32m     34\u001b[0m checker \u001b[38;5;241m=\u001b[39m BertChecker()\n\u001b[1;32m---> 35\u001b[0m checker\u001b[38;5;241m.\u001b[39mfrom_pretrained()\n\u001b[0;32m     36\u001b[0m checker\u001b[38;5;241m.\u001b[39mcorrect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI luk foward to receving your reply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\neuspell\\corrector_subwordbert.py:49\u001b[0m, in \u001b[0;36mCorrectorSubwordBert.from_pretrained\u001b[1;34m(self, ckpt_path, vocab, weights)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_path \u001b[38;5;241m=\u001b[39m weights \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading pretrained weights from path:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m load_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_path, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\neuspell\\seq_modeling\\subwordbert.py:23\u001b[0m, in \u001b[0;36mload_pretrained\u001b[1;34m(model, checkpoint_path, optimizer, device)\u001b[0m\n\u001b[0;32m     21\u001b[0m     map_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model params from checkpoint dir: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m checkpoint_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pth.tar\u001b[39m\u001b[38;5;124m\"\u001b[39m), map_location\u001b[38;5;241m=\u001b[39mmap_location)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# print(f\"previously model saved at : {checkpoint_data['epoch_id']}\")\u001b[39;00m\n\u001b[0;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[13], line 28\u001b[0m, in \u001b[0;36mcustom_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m custom_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_legacy_load(f, map_location, CustomUnpickler, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "Cell \u001b[1;32mIn[13], line 29\u001b[0m, in \u001b[0;36mcustom_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m custom_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_legacy_load(f, map_location, CustomUnpickler, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\torch\\serialization.py:1141\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m deserialized_objects: Dict[\u001b[38;5;28mint\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1139\u001b[0m restore_location \u001b[38;5;241m=\u001b[39m _get_restore_location(map_location)\n\u001b[1;32m-> 1141\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mUnpicklerWrapper\u001b[39;00m(pickle_module\u001b[38;5;241m.\u001b[39mUnpickler):  \u001b[38;5;66;03m# type: ignore[name-defined]\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_class\u001b[39m(\u001b[38;5;28mself\u001b[39m, mod_name, name):\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStorage\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'CustomUnpickler' has no attribute 'Unpickler'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "from functools import partial\n",
    "from neuspell import BertChecker\n",
    "\n",
    "# Adjust pickle.load with encoding\n",
    "pickle.load = partial(pickle.load, encoding=\"latin1\")\n",
    "pickle.Unpickler = partial(pickle._Unpickler, encoding=\"latin1\")\n",
    "\n",
    "# Custom Unpickler to handle encoding\n",
    "class CustomUnpickler(pickle._Unpickler):\n",
    "    def __init__(self, file, *args, **kwargs):\n",
    "        kwargs['encoding'] = 'latin1'\n",
    "        super().__init__(file, *args, **kwargs)\n",
    "\n",
    "    def find_class(self, module, name):\n",
    "        # Handle the 'code' type properly\n",
    "        if module == \"builtins\" and name == \"code\":\n",
    "            import types\n",
    "            return types.CodeType\n",
    "        return super().find_class(module, name)\n",
    "\n",
    "# Override torch load to use the custom Unpickler\n",
    "def custom_load(f, map_location=None, pickle_module=pickle, **pickle_load_args):\n",
    "    if isinstance(f, str):\n",
    "        with open(f, 'rb') as opened_file:\n",
    "            return custom_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
    "    return torch.serialization._legacy_load(f, map_location, CustomUnpickler, **pickle_load_args)\n",
    "\n",
    "torch.load = custom_load\n",
    "\n",
    "# Your existing code to initialize and use the checker\n",
    "checker = BertChecker()\n",
    "checker.from_pretrained()\n",
    "checker.correct(\"I luk foward to receving your reply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocab from path:d:\\anaconda\\Lib\\site-packages\\neuspell\\../data/checkpoints/subwordbert-probwordnoise\\vocab.pkl\n",
      "initializing model\n",
      "SubwordBert(\n",
      "  (bert_dropout): Dropout(p=0.2, inplace=False)\n",
      "  (bert_model): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dense): Linear(in_features=768, out_features=100002, bias=True)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "185211810\n",
      "loading pretrained weights from path:d:\\anaconda\\Lib\\site-packages\\neuspell\\../data/checkpoints/subwordbert-probwordnoise\n",
      "Loading model params from checkpoint dir: d:\\anaconda\\Lib\\site-packages\\neuspell\\../data/checkpoints/subwordbert-probwordnoise\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'UnpicklerWrapper' has no attribute 'Unpickler'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\" select spell checkers & load \"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m checker \u001b[38;5;241m=\u001b[39m BertChecker()\n\u001b[1;32m---> 12\u001b[0m checker\u001b[38;5;241m.\u001b[39mfrom_pretrained()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\" spell correction \"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m checker\u001b[38;5;241m.\u001b[39mcorrect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI luk foward to receving your reply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\neuspell\\corrector_subwordbert.py:49\u001b[0m, in \u001b[0;36mCorrectorSubwordBert.from_pretrained\u001b[1;34m(self, ckpt_path, vocab, weights)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_path \u001b[38;5;241m=\u001b[39m weights \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading pretrained weights from path:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m load_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_path, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\neuspell\\seq_modeling\\subwordbert.py:23\u001b[0m, in \u001b[0;36mload_pretrained\u001b[1;34m(model, checkpoint_path, optimizer, device)\u001b[0m\n\u001b[0;32m     21\u001b[0m     map_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model params from checkpoint dir: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m checkpoint_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pth.tar\u001b[39m\u001b[38;5;124m\"\u001b[39m), map_location\u001b[38;5;241m=\u001b[39mmap_location)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# print(f\"previously model saved at : {checkpoint_data['epoch_id']}\")\u001b[39;00m\n\u001b[0;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[57], line 24\u001b[0m, in \u001b[0;36mcustom_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m---> 24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m custom_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_legacy_load(f, map_location, UnpicklerWrapper, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "Cell \u001b[1;32mIn[57], line 25\u001b[0m, in \u001b[0;36mcustom_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m custom_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_legacy_load(f, map_location, UnpicklerWrapper, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\torch\\serialization.py:1053\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1049\u001b[0m deserialized_objects: Dict[\u001b[38;5;28mint\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1051\u001b[0m restore_location \u001b[38;5;241m=\u001b[39m _get_restore_location(map_location)\n\u001b[1;32m-> 1053\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mUnpicklerWrapper\u001b[39;00m(pickle_module\u001b[38;5;241m.\u001b[39mUnpickler):  \u001b[38;5;66;03m# type: ignore[name-defined]\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_class\u001b[39m(\u001b[38;5;28mself\u001b[39m, mod_name, name):\n\u001b[0;32m   1056\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStorage\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'UnpicklerWrapper' has no attribute 'Unpickler'"
     ]
    }
   ],
   "source": [
    "import neuspell\n",
    "from neuspell import BertChecker\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "# Adjust pickle.load with encoding\n",
    "pickle_load_with_encoding = partial(pickle.load, encoding=\"latin1\")\n",
    "pickle.Unpickler = lambda f: pickle._Unpickler(f, encoding=\"latin1\")\n",
    "\n",
    "\"\"\" select spell checkers & load \"\"\"\n",
    "checker = BertChecker()\n",
    "checker.from_pretrained()\n",
    "\n",
    "\"\"\" spell correction \"\"\"\n",
    "checker.correct(\"I luk foward to receving your reply\")\n",
    "# → \"I look forward to receiving your reply\"\n",
    "checker.correct_strings([\"I luk foward to receving your reply\", ])\n",
    "# → [\"I look forward to receiving your reply\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_refer = pd.read_csv('./data/describe_sum_carrier.csv')\n",
    "data_query = pd.read_csv(\"./data/describe_category_carrier.csv\")\n",
    "data_train = pd.read_csv(\"./data/corpus_short.csv\")\n",
    "# 随机提取10%的数据样本\n",
    "data_change = data_train.sample(frac=0.1, random_state=1).reset_index(drop=True)\n",
    "data_mask = data_train.sample(frac=0.1, random_state=2).reset_index(drop=True)\n",
    "data_expand = data_train.sample(frac=0.1, random_state=3).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_query_change = change_data(data_query.copy())\n",
    "data_query_mask = mask_data(data_query.copy())\n",
    "data_query_expand = expand_data(data_refer, data_query.copy())\n",
    "\n",
    "data_query_change.to_csv('./data/describe_category_carrier_change.csv', index=False)\n",
    "data_query_mask.to_csv('./data/describe_category_carrier_mask.csv', index=False)\n",
    "data_query_expand.to_csv('./data/describe_category_carrier_expand.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_query_change , data_query_mask, data_query_expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_query = pd.read_csv(\"./data/describe_category_carrier.csv\")\n",
    "data_example = data_query.sample(frac=0.1, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数据进行处理\n",
    "data_change_noise = change_data(data_change.reset_index(drop=True))\n",
    "data_mask_noise = mask_data(data_mask.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将数据与原数据合并\n",
    "data01 = pd.concat([data_train, data_change_noise], axis=0)\n",
    "data02 = pd.concat([data01, data_mask_noise], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data02.to_csv('./data/corpus_short_noise.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from sentence_transformers import SentenceTransformer, util, models\n",
    "import torch\n",
    "import pandas as pd\n",
    "import preprocessor\n",
    "import evaluation_SBERT\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "model_idx = 5\n",
    "file_idx = 1\n",
    "max_seq_length = 512\n",
    "\n",
    "\n",
    "# 实例化文本预处理类\n",
    "p = preprocessor.EnglishPreProcessor()\n",
    "\n",
    "\n",
    "def get_model(model_save_path):\n",
    "    model = SentenceTransformer(model_save_path)\n",
    "    return model\n",
    "    \n",
    "def evaluation_model(model, ndcg = True, mean_ap = True, topk = 10, file_idx = 1):\n",
    "    if os.path.exists('./data/refer_embeddings.pkl'):\n",
    "        with open('./data/refer_embeddings.pkl', 'rb') as f:\n",
    "            embeddings2 = pickle.load(f)\n",
    "    else:\n",
    "        # 导入语料库\n",
    "        data = pd.read_csv('./data/describe_sum_carrier.csv', encoding = 'utf-8')\n",
    "        data = pd.DataFrame(data)\n",
    "        refer = [p(sentence) if len(sentence)<=512 else p(sentence[:128] + sentence[-384:]) \n",
    "                for sentence in data['narr_accf']]\n",
    "        # 创建语料库的句向量\n",
    "        embeddings2 = model.encode(refer, convert_to_tensor = True)\n",
    "        pickle.dump(embeddings2, open('./data/refer_embeddings.pkl', 'wb'))\n",
    "    # 基于NDCG@N的评价方法\n",
    "    if ndcg:\n",
    "        avg_ndcg = evaluation_SBERT.cal_ndcg(model, embeddings2, topk)\n",
    "        print ('Dataset %s ndcg: %s' % (file_idx, avg_ndcg))\n",
    "    # 基于MAP的评价方法\n",
    "    if mean_ap:\n",
    "        mean_ap = evaluation_SBERT.cal_map(model, embeddings2, topk)\n",
    "        print ('Dataset %s map: %s' % (file_idx, mean_ap))\n",
    "        \n",
    "    return avg_ndcg, mean_ap\n",
    "\n",
    "def evaluation_model_n(model, n = 100, file_idx = 1):\n",
    "    if os.path.exists('./data/refer_embeddings.pkl'):\n",
    "        with open('./data/refer_embeddings.pkl', 'rb') as f:\n",
    "            embeddings2 = pickle.load(f)\n",
    "    else:\n",
    "        # 导入语料库\n",
    "        data = pd.read_csv('./data/describe_sum_carrier.csv', encoding = 'utf-8')\n",
    "        data = pd.DataFrame(data)\n",
    "        refer = [p(sentence) if len(sentence)<=512 else p(sentence[:128] + sentence[-384:]) \n",
    "                for sentence in data['narr_accf']]\n",
    "        # 创建语料库的句向量\n",
    "        embeddings2 = model.encode(refer, convert_to_tensor = True)\n",
    "        pickle.dump(embeddings2, open('./data/refer_embeddings.pkl', 'wb'))\n",
    "    # 灵敏度测试：取前n个评价的影响\n",
    "    score_ndcg = evaluation_SBERT.cal_ndcg(model, embeddings2, topk = 100, repeat=True)\n",
    "    score_map = evaluation_SBERT.cal_map(model, embeddings2, topk = 100, repeat=True)\n",
    "    # 保存评价结果\n",
    "    ndcg = pd.DataFrame(pd.read_csv('./result_noise/ndcg.csv', encoding = 'utf-8'))\n",
    "    map = pd.DataFrame(pd.read_csv('./result_noise/map.csv', encoding = 'utf-8'))\n",
    "    ndcg['Dataset ' + str(file_idx)] = score_ndcg\n",
    "    map['Dataset ' + str(file_idx)] = score_map\n",
    "    ndcg.to_csv('./result_noise/ndcg.csv', index=0)\n",
    "    map.to_csv('./result_noise/map.csv', index=0)\n",
    "    return score_ndcg, score_map\n",
    "\n",
    "def predict(model, queries, queries_ori = None, nosie = False, n = 10):\n",
    "    \"\"\"\n",
    "    :param queries : 查询语句集, list, [str1, str2, ...]\n",
    "    :\n",
    "    \"\"\"\n",
    "    # 导入并预处理语料库，创建语料库的句向量\n",
    "    data = pd.read_csv('./data/describe_sum_carrier.csv', encoding = 'utf-8')\n",
    "    data = pd.DataFrame(data)\n",
    "    keyword = data['Occurrence_description_sum'].tolist()\n",
    "    ntsb_no = data['ntsb_no'].tolist()\n",
    "    refer = data['narr_accf'].tolist()\n",
    "    if os.path.exists('./data/refer_embeddings.pkl'):\n",
    "        with open('./data/refer_embeddings.pkl', 'rb') as f:\n",
    "            refer_embeddings = pickle.load(f)\n",
    "    else:\n",
    "        refer_new = [p(sentence) if len(sentence)<=512 else p(sentence[:128] + sentence[-384:]) \n",
    "                    for sentence in refer]\n",
    "        refer_embeddings = model.encode(refer_new, convert_to_tensor = True)\n",
    "        pickle.dump(refer_embeddings, open('./data/refer_embeddings.pkl', 'wb'))\n",
    "    # 事故发生阶段+原因\n",
    "    if os.path.exists('./data/embeddings0.pkl'):\n",
    "        with open('./data/embeddings0.pkl', 'rb') as f:\n",
    "            embeddings0 = pickle.load(f)\n",
    "    else:\n",
    "        dict = pd.read_csv('./data/describe_category_carrier.csv')\n",
    "        dict = [phase for phase in dict['Occurrence_description_sum']]\n",
    "        embeddings0 = model.encode(dict, convert_to_tensor = True)\n",
    "        #存为pkl文件\n",
    "        pickle.dump(embeddings0, open('./data/embeddings0.pkl', 'wb'))\n",
    "\n",
    "    embeddings2 = []\n",
    "    # 预处理查询语句集\n",
    "    for i in range(len(queries)):\n",
    "        query = queries[i]\n",
    "        # 输入的query\n",
    "        if (len(query) > 100):\n",
    "            query = query.split('.')\n",
    "            query = [sentence.split(',') for sentence in query]\n",
    "            query = [p(sentence) for sentences in query for sentence in sentences if len(sentence) > 2]\n",
    "        embeddings2.append(model.encode(query, convert_to_tensor = True))\n",
    "    # 基于余弦相似度为每条查询语句寻找前n个相似的语料库文本\n",
    "    top_k = min(n, refer_embeddings.shape[0])\n",
    "    total_top_results = []\n",
    "    contents = []\n",
    "    for i in range(len(queries)):\n",
    "        # 创建查询语句的句向量\n",
    "        query_embedding = fliter(embeddings0, embeddings2[i])\n",
    "        # 通过余弦相似度和torch.topk获得前n个最高分数\n",
    "        cos_scores = util.cos_sim(query_embedding, refer_embeddings)[0]\n",
    "        top_results = torch.topk(cos_scores, k = top_k)\n",
    "        total_top_results.append(top_results)\n",
    "        # 记录top_results对应的文本内容\n",
    "        content = [keyword[idx] for idx in top_results[1]]\n",
    "        contents.append(content)\n",
    "        #eval_pr\n",
    "        if not nosie:\n",
    "            eval_pr = eval_predict(queries[i], top_results)\n",
    "        else:\n",
    "            eval_pr = eval_predict(queries_ori[i], top_results)\n",
    "        # # 打印预测结果\n",
    "        # print(\"\\n\\n========================================================\\n\\n\")\n",
    "        # print(\"查询语句:\", queries[i])\n",
    "        # print(\"\\n语料库中前 {0} 个最相似的语句:\".format(top_k))\n",
    "        # j = 1\n",
    "        # for score, idx in zip(top_results[0], top_results[1]):\n",
    "        #     print(\"【%s】【NTSB NO.%s】\" % (j, ntsb_no[idx]), refer[idx].strip(),\n",
    "        #           \"(Score: {:.4f})\".format(score))\n",
    "        #     j += 1\n",
    "        print(\"(eval_pr: {:.4f})\".format(eval_pr/len(top_results[0])))\n",
    "    return contents\n",
    "\n",
    "def eval_predict(query,results):\n",
    "    # 导入语料库\n",
    "    data = pd.read_csv('./data/describe_sum_carrier.csv', encoding = 'utf-8')\n",
    "    data = pd.DataFrame(data)\n",
    "    occurences = data['Occurrence_description_sum'].tolist()\n",
    "    eval_qr = 0\n",
    "    for score, idx in zip(results[0], results[1]):\n",
    "        occurence = occurences[idx]\n",
    "        if query in occurence:\n",
    "            eval_qr += 1\n",
    "    return eval_qr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from sentence_transformers import SentenceTransformer, util, models\n",
    "import torch\n",
    "import pandas as pd\n",
    "import evaluation_SBERT\n",
    "import torch\n",
    "\n",
    "\n",
    "model_idx = 5\n",
    "file_idx = 1\n",
    "max_seq_length = 512\n",
    "\n",
    "\n",
    "# 实例化文本预处理类\n",
    "p = preprocessor.EnglishPreProcessor()\n",
    "\n",
    "\n",
    "def get_model(model_save_path):\n",
    "    model = SentenceTransformer(model_save_path)\n",
    "    return model\n",
    "    \n",
    "def evaluation_model(model, ndcg = True, mean_ap = True, topk = 10, file_idx = 1):\n",
    "    # 导入语料库\n",
    "    data = pd.read_csv('./data/describe_sum_carrier.csv', encoding = 'utf-8')\n",
    "    data = pd.DataFrame(data)\n",
    "    refer = [p(sentence) if len(sentence)<=512 else p(sentence[:128] + sentence[-384:]) \n",
    "             for sentence in data['narr_accf']]\n",
    "    # 创建语料库的句向量\n",
    "    embeddings2 = model.encode(refer, convert_to_tensor = True)\n",
    "    # 基于NDCG@N的评价方法\n",
    "    if ndcg:\n",
    "        avg_ndcg = evaluation_SBERT.cal_ndcg(model, embeddings2, topk)\n",
    "        print ('Dataset %s ndcg: %s' % (file_idx, avg_ndcg))\n",
    "    # 基于MAP的评价方法\n",
    "    if mean_ap:\n",
    "        mean_ap = evaluation_SBERT.cal_map(model, embeddings2, topk)\n",
    "        print ('Dataset %s map: %s' % (file_idx, mean_ap))\n",
    "        \n",
    "    return avg_ndcg, mean_ap\n",
    "\n",
    "def evaluation_model_n(n = 100,file_idx = 1):\n",
    "    # 导入语料库\n",
    "    data = pd.read_csv('./data/describe_sum_carrier.csv', encoding = 'utf-8')\n",
    "    data = pd.DataFrame(data)\n",
    "    refer = [p(sentence) if len(sentence)<=512 else p(sentence[:128] + sentence[-384:]) \n",
    "             for sentence in data['narr_accf']]\n",
    "    # 创建语料库的句向量\n",
    "    embeddings2 = model.encode(refer, convert_to_tensor = True)\n",
    "    # 灵敏度测试：取前n个评价的影响\n",
    "    score_ndcg = evaluation_SBERT.cal_ndcg(model, embeddings2, repeat=True)\n",
    "    score_map = evaluation_SBERT.cal_map(model, embeddings2, repeat=True)\n",
    "    # 保存评价结果\n",
    "    ndcg = pd.DataFrame(pd.read_csv('./result_noise/ndcg_change.csv', encoding = 'utf-8'))\n",
    "    map = pd.DataFrame(pd.read_csv('./result_noise/map_change.csv', encoding = 'utf-8'))\n",
    "    ndcg['Dataset ' + str(file_idx)] = score_ndcg\n",
    "    map['Dataset ' + str(file_idx)] = score_map\n",
    "    ndcg.to_csv('./result_noise/ndcg_change.csv', index=0)\n",
    "    map.to_csv('./result_noise/map_change.csv', index=0)\n",
    "    return score_ndcg, score_map\n",
    "\n",
    "def predict(model, queries, queries_ori = None, nosie = False, n = 10):\n",
    "    \"\"\"\n",
    "    :param queries : 查询语句集, list, [str1, str2, ...]\n",
    "    :\n",
    "    \"\"\"\n",
    "    # 导入并预处理语料库，创建语料库的句向量\n",
    "    data = pd.read_csv('./data/describe_sum_carrier.csv', encoding = 'utf-8')\n",
    "    data = pd.DataFrame(data)\n",
    "    keyword = data['Occurrence_description_sum'].tolist()\n",
    "    ntsb_no = data['ntsb_no'].tolist()\n",
    "    refer = data['narr_accf'].tolist()\n",
    "    refer_new = [p(sentence) if len(sentence)<=512 else p(sentence[:128] + sentence[-384:]) \n",
    "                 for sentence in refer]\n",
    "    refer_embeddings = model.encode(refer_new, convert_to_tensor = True)\n",
    "    # 预处理查询语句集\n",
    "    queries_new = [p(sentence) if len(sentence)<=512 else p(sentence[:128] + sentence[-384:]) \n",
    "                   for sentence in queries]\n",
    "    # 基于余弦相似度为每条查询语句寻找前n个相似的语料库文本\n",
    "    top_k = min(n, len(refer))\n",
    "    total_top_results = []\n",
    "    contents = []\n",
    "    for i in range(len(queries)):\n",
    "        # 创建查询语句的句向量\n",
    "        query_embedding = model.encode(queries_new[i], convert_to_tensor = True)\n",
    "        # 通过余弦相似度和torch.topk获得前n个最高分数\n",
    "        cos_scores = util.cos_sim(query_embedding, refer_embeddings)[0]\n",
    "        top_results = torch.topk(cos_scores, k = top_k)\n",
    "        total_top_results.append(top_results)\n",
    "        # 记录top_results对应的文本内容\n",
    "        content = [keyword[idx] for idx in top_results[1]]\n",
    "        contents.append(content)\n",
    "        #eval_pr\n",
    "        if not nosie:\n",
    "            eval_pr = eval_predict(queries[i], top_results)\n",
    "        else:\n",
    "            eval_pr = eval_predict(queries_ori[i], top_results)\n",
    "        # # 打印预测结果\n",
    "        # print(\"\\n\\n========================================================\\n\\n\")\n",
    "        # print(\"查询语句:\", queries[i])\n",
    "        # print(\"\\n语料库中前 {0} 个最相似的语句:\".format(top_k))\n",
    "        # j = 1\n",
    "        # for score, idx in zip(top_results[0], top_results[1]):\n",
    "        #     print(\"【%s】【NTSB NO.%s】\" % (j, ntsb_no[idx]), refer[idx].strip(),\n",
    "        #           \"(Score: {:.4f})\".format(score))\n",
    "        #     j += 1\n",
    "        print(\"(eval_pr: {:.4f})\".format(eval_pr/len(top_results[0])))\n",
    "    return contents\n",
    "\n",
    "def eval_predict(query,results):\n",
    "    # 导入语料库\n",
    "    data = pd.read_csv('./data/describe_sum_carrier.csv', encoding = 'utf-8')\n",
    "    data = pd.DataFrame(data)\n",
    "    occurences = data['Occurrence_description_sum'].tolist()\n",
    "    eval_qr = 0\n",
    "    for score, idx in zip(results[0], results[1]):\n",
    "        occurence = occurences[idx]\n",
    "        if query in occurence:\n",
    "            eval_qr += 1\n",
    "    return eval_qr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Occurrence_description_sum</th>\n",
       "      <th>code_sum</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>Standing - engine(s) not operating Miscellaneo...</td>\n",
       "      <td>504+430,504+430</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>Takeoff Airframe/component/system failure/malf...</td>\n",
       "      <td>520+130,540+130</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>Takeoff - initial climb Explosion,Takeoff - in...</td>\n",
       "      <td>522+172,522+352,571+180</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Descent Near collision between aircraft</td>\n",
       "      <td>550+280</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Landing - flare/touchdown In flight collision ...</td>\n",
       "      <td>571+220</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>Maneuvering Abrupt maneuver,Maneuvering Abrupt...</td>\n",
       "      <td>580+100,580+100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>Taxi - pushback/tow Propeller/rotor contact to...</td>\n",
       "      <td>511+370</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>Taxi - to takeoff On ground/water collision wi...</td>\n",
       "      <td>512+310,512+130,574+232</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>Taxi Miscellaneous/other,Takeoff Overrun,Takeo...</td>\n",
       "      <td>510+430,520+340,520+230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>Cruise Airframe/component/system failure/malfu...</td>\n",
       "      <td>540+130,540+171,570+430</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Occurrence_description_sum  \\\n",
       "677  Standing - engine(s) not operating Miscellaneo...   \n",
       "653  Takeoff Airframe/component/system failure/malf...   \n",
       "530  Takeoff - initial climb Explosion,Takeoff - in...   \n",
       "104            Descent Near collision between aircraft   \n",
       "179  Landing - flare/touchdown In flight collision ...   \n",
       "..                                                 ...   \n",
       "358  Maneuvering Abrupt maneuver,Maneuvering Abrupt...   \n",
       "641  Taxi - pushback/tow Propeller/rotor contact to...   \n",
       "575  Taxi - to takeoff On ground/water collision wi...   \n",
       "665  Taxi Miscellaneous/other,Takeoff Overrun,Takeo...   \n",
       "615  Cruise Airframe/component/system failure/malfu...   \n",
       "\n",
       "                    code_sum  count  \n",
       "677          504+430,504+430      1  \n",
       "653          520+130,540+130      1  \n",
       "530  522+172,522+352,571+180      1  \n",
       "104                  550+280      3  \n",
       "179                  571+220      2  \n",
       "..                       ...    ...  \n",
       "358          580+100,580+100      1  \n",
       "641                  511+370      1  \n",
       "575  512+310,512+130,574+232      1  \n",
       "665  510+430,520+340,520+230      1  \n",
       "615  540+130,540+171,570+430      1  \n",
       "\n",
       "[68 rows x 3 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.5000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.6000)\n",
      "(eval_pr: 0.4000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.4000)\n",
      "(eval_pr: 0.3000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.4000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.3000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n"
     ]
    }
   ],
   "source": [
    "# 相关参数设定\n",
    "model_idx = 5\n",
    "file_idx = 1\n",
    "max_seq_length = 512\n",
    "model_save_path = './model/model_sbert_supervised_' + str(model_idx)\n",
    "\n",
    "# 加载模型\n",
    "model = get_model(model_save_path)\n",
    "\n",
    "queries = data_example['Occurrence_description_sum'].tolist()\n",
    "contents = predict(model, queries, n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 1.0000)\n",
      "(eval_pr: 0.4000)\n",
      "(eval_pr: 0.5000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.6000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.5000)\n",
      "(eval_pr: 0.3000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.4000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.6000)\n",
      "(eval_pr: 0.8000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.7000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.6000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.5000)\n"
     ]
    }
   ],
   "source": [
    "# 相关参数设定\n",
    "model_idx = 1\n",
    "file_idx = 1\n",
    "max_seq_length = 512\n",
    "model_save_path = './model/model.sbert_noise_' + str(model_idx)\n",
    "\n",
    "# 加载模型\n",
    "model = get_model(model_save_path)\n",
    "\n",
    "queries = data_change_noise['Occurrence_description_sum'].tolist()\n",
    "queries_ori = data_change['Occurrence_description_sum'].tolist()\n",
    "contents = predict(model, queries, queries_ori, nosie= True, n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.9000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.6000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.7000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.5000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.6000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.4000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.6000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.3000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.3000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "# 相关参数设定\n",
    "model_idx = 5\n",
    "file_idx = 1\n",
    "max_seq_length = 512\n",
    "model_save_path = './model/model_sbert_supervised_' + str(model_idx)\n",
    "\n",
    "# 加载模型\n",
    "model = get_model(model_save_path)\n",
    "\n",
    "queries = data_change_noise['Occurrence_description_sum'].tolist()\n",
    "contents = predict(model, queries, n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(eval_pr: 0.3000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.4000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.5000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.3000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 1.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.3000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.4000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.1000)\n"
     ]
    }
   ],
   "source": [
    "# 相关参数设定\n",
    "model_idx = 1\n",
    "model_save_path = './model/model.sbert_noise_' + str(model_idx)\n",
    "\n",
    "# 加载模型\n",
    "model = get_model(model_save_path)\n",
    "\n",
    "queries = data_mask_noise['Occurrence_description_sum'].tolist()\n",
    "queries_ori = data_mask['Occurrence_description_sum'].tolist()\n",
    "contents = predict(model, queries, queries_ori, nosie=True, n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.4000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.8000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.3000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.4000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.1000)\n",
      "(eval_pr: 0.2000)\n",
      "(eval_pr: 0.1000)\n"
     ]
    }
   ],
   "source": [
    "# 相关参数设定\n",
    "model_idx = 5\n",
    "model_save_path = './model/model_sbert_supervised_' + str(model_idx)\n",
    "\n",
    "# 加载模型\n",
    "model = get_model(model_save_path)\n",
    "\n",
    "queries = data_mask_noise['Occurrence_description_sum'].tolist()\n",
    "contents = predict(model, queries, n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "# 相关参数设定\n",
    "model_idx = 1\n",
    "model_save_path = './model/model.sbert_noise_' + str(model_idx)\n",
    "\n",
    "# 加载模型\n",
    "model = get_model(model_save_path)\n",
    "\n",
    "queries = data_expand_noise['Occurrence_description_sum'].tolist()\n",
    "queries_ori = data_expand['Occurrence_description_sum'].tolist()\n",
    "contents = predict(model, queries, n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n",
      "(eval_pr: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "# 相关参数设定\n",
    "model_idx = 5\n",
    "model_save_path = './model/model_sbert_supervised_' + str(model_idx)\n",
    "\n",
    "# 加载模型\n",
    "model = get_model(model_save_path)\n",
    "\n",
    "queries = data_expand_noise['Occurrence_description_sum'].tolist()\n",
    "contents = predict(model, queries, n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相关参数设定\n",
    "model_idx = 1\n",
    "model_save_path = './model/model.sbert_noise_' + str(model_idx)\n",
    "\n",
    "# 加载模型\n",
    "model = get_model(model_save_path)\n",
    "\n",
    "score_ndcg, score_map = evaluation_model_n(model, n = 100, file_idx = model_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# 相关参数设定\n",
    "model_idx = 5\n",
    "model_save_path = './model/model_sbert_supervised_' + str(model_idx)\n",
    "\n",
    "# 加载模型\n",
    "model = get_model(model_save_path)\n",
    "\n",
    "score_ndcg, score_map = evaluation_model_n(model, n = 100, file_idx = model_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理长文本，将其按照句号划分，组成分块，每个分块的长度不超过512个字符\n",
    "def sliding_window(text, max_length=512, overlap=1):\n",
    "    sentences = text.split(\".\")\n",
    "    sentences = [s.strip() + \".\" for s in sentences if s.strip()] # Remove empty sentences and add back the period\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if len(chunk) + len(sentence) <= max_length:\n",
    "            chunk += sentence\n",
    "        else:\n",
    "            if len(chunk) == 0:\n",
    "                chunk = sentence[:max_length]\n",
    "            chunks.append(chunk)\n",
    "            chunk = \"\"\n",
    "            \n",
    "            overlap_sen = \"\"\n",
    "            if i > 0 and overlap > 0:\n",
    "                if i < overlap:\n",
    "                    overlap_sen = \"\".join(sentences[:i])\n",
    "                else:\n",
    "                    overlap_sen = \"\".join(sentences[i-overlap:i])\n",
    "            chunk = overlap_sen + sentence\n",
    "            \n",
    "    if chunk:\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "text = \"The captain briefed a no go-around for a night visual approach to a Special Airport. The approach was not stabilized, and the airspeed decreased to the point of a stall. The airplane struck the runway in a nose high pitch attitude, on the aft fuselage, and settled on the landing gear. The first officer made initial callouts of slow airspeed and then stopped when the captain failed to respond to her callouts.  After landing, the airplane was taxied to the gate where a post flight inspection limited to the main landing gear did not find the damage.  When interviewed, the captain reported that she briefed \"\"no go-around because no takeoffs were authorized on the runway at night or in IMC conditions; however, the first officer knew this was incorrect, but did not challenge the captain.  Both pilots had received CRM training, which included crewmember assertiveness, methods of fostering crew input, and situational awareness, and training on special use airports; however it was not followed by either pilot.  The captain's handling of the airplane was outside the parameters specified in the company manuals.  Both pilots were described to having good flying skills.  The captain said the first officer was passive and quiet.  The first officer reported the captain was defensive and did not take criticism very well.  A definition of stabilized approach criteria was not found in the company manuals.  An FAA Advisory Circular dated August 10, 2000 defined stabilized approach criteria, and actions to be taken if the approach was not stabilized.,Landing Loss of control - in flight,Landing - flare/touchdown Hard landing。The captain briefed a no go-around for a night visual approach to a Special Airport.  The approach was not  stabilized, and the airspeed decreased to the point of a stall.  The airplane struck the runway in a nose high pitch attitude, on the aft fuselage, and settled on the landing gear.  The first officer made initial callouts of slow airspeed and then stopped when the captain failed to respond to her callouts.  After landing, the airplane was taxied to the gate where a post flight inspection limited to the main landing gear did not find the damage.  When interviewed, the captain reported that she briefed no go-around because no takeoffs were authorized on the runway at night or in IMC conditions; however, the first officer knew this was incorrect, but did not challenge the captain.  Both pilots had received CRM training, which included crewmember assertiveness, methods of fostering crew input, and situational awareness, and training on special use airports; however it was not followed by either pilot.  The captain's handling of the airplane was outside the parameters specified in the company manuals.  Both pilots were described to having good flying skills.  The captain said the first officer was passive and quiet.  The first officer reported the captain was defensive and did not take criticism very well.  A definition of stabilized approach criteria was not found in the company manuals.  An FAA Advisory Circular dated August 10, 2000 defined stabilized approach criteria, and actions to be taken if the approach was not stabilized.\"\n",
    "chunks = sliding_window(text)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: recovery from a bounced landing was delayed by the pilot, which resulted in an in-flight collision with a tree and soft terrain, and subsequent nose over and nose over.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# 加载预训练的BART模型和分词器\n",
    "model_name = './model/fine_tuned_bart_narr32'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def summarize(text):\n",
    "    # 将输入文本进行分词\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors='pt', max_length=1024, truncation=True)\n",
    "    \n",
    "    # 使用模型生成摘要\n",
    "    summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    \n",
    "    # 将生成的摘要解码为文本\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# 示例文本\n",
    "text = \"the pilot was flying over a prospective landing site located atop a mountain ridge, when he inadvertently allowed the wheels of the airplane to touchdown. the airplane bounced, and the pilot applied full engine power in an attempt to abort the landing. during the aborted landing, the right wing struck a tree at the departure end of the site, and the airplane descended onto soft tundra. the airplane nosed over and received damage to the right wing and the right wing lift strut. the pilot noted that there were no preaccident mechanical anomalies with the airplane.\"\n",
    "\n",
    "# 生成摘要\n",
    "summary = summarize(text)\n",
    "print(\"Summary:\", summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
